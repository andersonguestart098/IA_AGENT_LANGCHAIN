import os
from langchain_community.chat_models import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain_core.documents import Document
from app.services.embeddings import embedding_model
from app.generated.client import Prisma
from langchain.output_parsers import OutputFixingParser


async def setup_rag_chain(sessao_token: str):
    prisma = Prisma()
    await prisma.connect()

    # üîπ Recupera a sess√£o e hist√≥rico recente da conversa
    sessao = await prisma.sessao.find_unique(where={"token": sessao_token})
    historico = await prisma.fluxoconversa.find_many(
        where={"sessaoId": sessao.id},
        order={"id": "asc"},
        take=5
    )
    eh_primeira_interacao = len(historico) == 0
    chat_history = [(h.pedido, h.resposta) for h in historico if h.resposta]

    # üîπ Instancia LLM (Mistral via OpenAI compat√≠vel)
    llm = ChatOpenAI(
        model="mistral-large-latest",
        openai_api_key=os.getenv("MISTRAL_API_KEY"),
        base_url="https://api.mistral.ai/v1",
        temperature=0.3
    )

    # üîπ Cadeia de classifica√ß√£o de inten√ß√£o
    intencao_prompt = PromptTemplate.from_template("""
Voc√™ √© um classificador de inten√ß√£o para uma assistente virtual da Cemear (pisos, divis√≥rias e solu√ß√µes ac√∫sticas).
Classifique a frase do cliente em UMA das seguintes categorias:
- SAUDACAO
- PEDIDO_ORCAMENTO
- PERGUNTA_PRODUTO
- VAGA_EMPREGO
- FORA_REGIAO
- CONTINUIDADE_FLUXO
- DESPEDIDA
- CONFIRMACAO
- OUTRO

Frase: {texto}
Categoria:
""")
    classificacao_chain = intencao_prompt | llm | StrOutputParser()

    # üîπ Prompt de Slot Filling estruturado
    slot_prompt = PromptTemplate.from_template("""
Dada a frase de um cliente, extraia as seguintes informa√ß√µes de forma estruturada:

- produto
- volume_aproximado
- localidade
- prazo

Se alguma informa√ß√£o n√£o estiver presente, use `null`.

Responda APENAS com um JSON v√°lido, exatamente neste formato:
{{
  "produto": string | null,
  "volume_aproximado": string | null,
  "localidade": string | null,
  "prazo": string | null
}}

Frase: {texto}
JSON:
""")

    # üîπ Cria parser com fallback para corrigir formatos
    slot_filling_chain = slot_prompt | llm | OutputFixingParser.from_llm(
        parser=JsonOutputParser(), llm=llm
    )

    # üîπ Carrega documentos com embeddings
    docs_db = await prisma.knowledgebase.find_many(where={"embedding": {"not": ""}})
    docs = [
        Document(
            page_content=doc.conteudo,
            metadata={"id": doc.id, "source": doc.origem}
        )
        for doc in docs_db
    ]
    vectorstore = FAISS.from_documents(docs, embedding_model)
    retriever = vectorstore.as_retriever(search_type="similarity", k=3)

    # üîπ Prompt do RAG com regras contextuais
    resposta_prompt = PromptTemplate.from_template("""
Voc√™ √© o atendente virtual da Cemear, especializada em pisos, divis√≥rias, brises e solu√ß√µes ac√∫sticas.

Regras:
- Responda SOMENTE com base no DOCUMENTO abaixo.
- Seja direto, educado e evite frases gen√©ricas como "como posso ajudar?" ou "entre em contato conosco".
- N√ÉO mencione hor√°rios comerciais, exceto se o cliente pedir.
- Apenas diga que vai encaminhar o pedido se a inten√ß√£o for PEDIDO_ORCAMENTO.
- Evite repetir sauda√ß√µes se n√£o for a primeira intera√ß√£o.
- Considere o HIST√ìRICO da conversa para responder com contexto.
- Se a resposta n√£o estiver clara no documento, diga: "Preciso consultar um especialista sobre isso."

HIST√ìRICO DE CONVERSA:
{chat_history}

DOCUMENTO:
{context}

PERGUNTA ATUAL:
{question}
""")

    # üîπ Cadeia principal de resposta com hist√≥rico e documentos
    rag_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True,
        combine_docs_chain_kwargs={
            "prompt": resposta_prompt
        }
    )

    # üîπ Retorna todos os componentes para uso no fluxo principal
    return {
        "prisma": prisma,
        "llm": llm,
        "classificacao_chain": classificacao_chain,
        "slot_filling_chain": slot_filling_chain,
        "rag_chain": rag_chain,
        "sessao": sessao,
        "eh_primeira_interacao": eh_primeira_interacao,
        "chat_history": chat_history
    }
